{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cc127867",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-27T19:46:00.116452Z",
     "iopub.status.busy": "2025-03-27T19:46:00.116120Z",
     "iopub.status.idle": "2025-03-27T19:46:03.330585Z",
     "shell.execute_reply": "2025-03-27T19:46:03.329886Z"
    },
    "id": "5db35d05",
    "papermill": {
     "duration": 3.220184,
     "end_time": "2025-03-27T19:46:03.332026",
     "exception": false,
     "start_time": "2025-03-27T19:46:00.111842",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5f526c4d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-27T19:46:03.339896Z",
     "iopub.status.busy": "2025-03-27T19:46:03.339541Z",
     "iopub.status.idle": "2025-03-27T19:46:03.346117Z",
     "shell.execute_reply": "2025-03-27T19:46:03.345554Z"
    },
    "id": "cMV0vTRfAN0I",
    "papermill": {
     "duration": 0.011328,
     "end_time": "2025-03-27T19:46:03.347474",
     "exception": false,
     "start_time": "2025-03-27T19:46:03.336146",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "31e411cd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-27T19:46:03.353635Z",
     "iopub.status.busy": "2025-03-27T19:46:03.353438Z",
     "iopub.status.idle": "2025-03-27T19:46:04.118959Z",
     "shell.execute_reply": "2025-03-27T19:46:04.118302Z"
    },
    "id": "c4bc7518",
    "papermill": {
     "duration": 0.77019,
     "end_time": "2025-03-27T19:46:04.120562",
     "exception": false,
     "start_time": "2025-03-27T19:46:03.350372",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "train_dataset = pd.read_csv('/kaggle/input/machine-translation-ioai/train.csv').values\n",
    "test_dataset = pd.read_csv('/kaggle/input/machine-translation-ioai/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e28e64d5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-27T19:46:04.127131Z",
     "iopub.status.busy": "2025-03-27T19:46:04.126909Z",
     "iopub.status.idle": "2025-03-27T19:58:57.159310Z",
     "shell.execute_reply": "2025-03-27T19:58:57.158291Z"
    },
    "papermill": {
     "duration": 773.037291,
     "end_time": "2025-03-27T19:58:57.160818",
     "exception": false,
     "start_time": "2025-03-27T19:46:04.123527",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "human 82\n",
      "iso 13\n",
      "Epoch [01/15]\n",
      "Training (9.0%) loss: 1.7726, accuracy: 28.27%\n",
      "Training (18.2%) loss: 1.0414, accuracy: 61.73%\n",
      "Training (27.3%) loss: 0.7140, accuracy: 71.73%\n",
      "Training (36.4%) loss: 0.6584, accuracy: 75.45%\n",
      "Training (45.6%) loss: 0.6103, accuracy: 76.82%\n",
      "Training (54.7%) loss: 0.6043, accuracy: 77.64%\n",
      "Training (63.8%) loss: 0.5824, accuracy: 77.55%\n",
      "Training (73.0%) loss: 0.5466, accuracy: 79.09%\n",
      "Training (82.1%) loss: 0.5309, accuracy: 80.45%\n",
      "Training (91.2%) loss: 0.4942, accuracy: 82.45%\n",
      "Validation loss: 0.4567, accuracy: 84.25%\n",
      "Epoch [02/15]\n",
      "Training (9.0%) loss: 0.4589, accuracy: 83.82%\n",
      "Training (18.2%) loss: 0.4588, accuracy: 83.64%\n",
      "Training (27.3%) loss: 0.4295, accuracy: 85.91%\n",
      "Training (36.4%) loss: 0.4240, accuracy: 85.82%\n",
      "Training (45.6%) loss: 0.3734, accuracy: 87.18%\n",
      "Training (54.7%) loss: 0.3743, accuracy: 87.91%\n",
      "Training (63.8%) loss: 0.3548, accuracy: 88.09%\n",
      "Training (73.0%) loss: 0.3207, accuracy: 88.73%\n",
      "Training (82.1%) loss: 0.3031, accuracy: 90.27%\n",
      "Training (91.2%) loss: 0.2790, accuracy: 91.55%\n",
      "Validation loss: 0.2655, accuracy: 91.61%\n",
      "Epoch [03/15]\n",
      "Training (9.0%) loss: 0.2861, accuracy: 91.36%\n",
      "Training (18.2%) loss: 0.3630, accuracy: 89.00%\n",
      "Training (27.3%) loss: 0.2651, accuracy: 90.45%\n",
      "Training (36.4%) loss: 0.2414, accuracy: 92.09%\n",
      "Training (45.6%) loss: 0.2178, accuracy: 93.64%\n",
      "Training (54.7%) loss: 0.2071, accuracy: 94.18%\n",
      "Training (63.8%) loss: 0.2164, accuracy: 93.00%\n",
      "Training (73.0%) loss: 0.1915, accuracy: 94.27%\n",
      "Training (82.1%) loss: 0.1804, accuracy: 94.18%\n",
      "Training (91.2%) loss: 0.1557, accuracy: 96.18%\n",
      "Validation loss: 0.1342, accuracy: 96.16%\n",
      "Epoch [04/15]\n",
      "Training (9.0%) loss: 0.1520, accuracy: 95.82%\n",
      "Training (18.2%) loss: 0.1298, accuracy: 96.09%\n",
      "Training (27.3%) loss: 0.1219, accuracy: 96.27%\n",
      "Training (36.4%) loss: 0.1015, accuracy: 97.64%\n",
      "Training (45.6%) loss: 0.0970, accuracy: 97.64%\n",
      "Training (54.7%) loss: 0.0899, accuracy: 97.27%\n",
      "Training (63.8%) loss: 0.1041, accuracy: 97.18%\n",
      "Training (73.0%) loss: 0.0950, accuracy: 97.09%\n",
      "Training (82.1%) loss: 0.0963, accuracy: 97.45%\n",
      "Training (91.2%) loss: 0.0753, accuracy: 98.55%\n",
      "Validation loss: 0.0573, accuracy: 98.52%\n",
      "Epoch [05/15]\n",
      "Training (9.0%) loss: 0.0797, accuracy: 97.64%\n",
      "Training (18.2%) loss: 0.0628, accuracy: 98.09%\n",
      "Training (27.3%) loss: 0.0510, accuracy: 98.91%\n",
      "Training (36.4%) loss: 0.0402, accuracy: 98.91%\n",
      "Training (45.6%) loss: 0.0406, accuracy: 98.82%\n",
      "Training (54.7%) loss: 0.0530, accuracy: 98.36%\n",
      "Training (63.8%) loss: 0.0478, accuracy: 98.55%\n",
      "Training (73.0%) loss: 0.0480, accuracy: 98.45%\n",
      "Training (82.1%) loss: 0.0464, accuracy: 98.91%\n",
      "Training (91.2%) loss: 0.0298, accuracy: 99.27%\n",
      "Validation loss: 0.0351, accuracy: 99.12%\n",
      "Epoch [06/15]\n",
      "Training (9.0%) loss: 0.0248, accuracy: 99.36%\n",
      "Training (18.2%) loss: 0.0220, accuracy: 99.55%\n",
      "Training (27.3%) loss: 0.0200, accuracy: 99.64%\n",
      "Training (36.4%) loss: 0.0182, accuracy: 99.73%\n",
      "Training (45.6%) loss: 0.0165, accuracy: 99.64%\n",
      "Training (54.7%) loss: 0.0166, accuracy: 99.55%\n",
      "Training (63.8%) loss: 0.0255, accuracy: 99.18%\n",
      "Training (73.0%) loss: 0.0170, accuracy: 99.64%\n",
      "Training (82.1%) loss: 0.0102, accuracy: 100.00%\n",
      "Training (91.2%) loss: 0.0090, accuracy: 100.00%\n",
      "Validation loss: 0.0091, accuracy: 99.93%\n",
      "Epoch [07/15]\n",
      "Training (9.0%) loss: 0.0147, accuracy: 99.82%\n",
      "Training (18.2%) loss: 0.0106, accuracy: 99.91%\n",
      "Training (27.3%) loss: 0.0077, accuracy: 100.00%\n",
      "Training (36.4%) loss: 0.0060, accuracy: 100.00%\n",
      "Training (45.6%) loss: 0.0064, accuracy: 100.00%\n",
      "Training (54.7%) loss: 0.0065, accuracy: 99.91%\n",
      "Training (63.8%) loss: 0.0081, accuracy: 99.82%\n",
      "Training (73.0%) loss: 0.0060, accuracy: 100.00%\n",
      "Training (82.1%) loss: 0.0042, accuracy: 100.00%\n",
      "Training (91.2%) loss: 0.0038, accuracy: 100.00%\n",
      "Validation loss: 0.0040, accuracy: 99.98%\n",
      "Epoch [08/15]\n",
      "Training (9.0%) loss: 0.0033, accuracy: 100.00%\n",
      "Training (18.2%) loss: 0.0051, accuracy: 99.91%\n",
      "Training (27.3%) loss: 0.0035, accuracy: 100.00%\n",
      "Training (36.4%) loss: 0.0029, accuracy: 100.00%\n",
      "Training (45.6%) loss: 0.0027, accuracy: 100.00%\n",
      "Training (54.7%) loss: 0.0029, accuracy: 100.00%\n",
      "Training (63.8%) loss: 0.0027, accuracy: 100.00%\n",
      "Training (73.0%) loss: 0.0028, accuracy: 100.00%\n",
      "Training (82.1%) loss: 0.0022, accuracy: 100.00%\n",
      "Training (91.2%) loss: 0.0023, accuracy: 100.00%\n",
      "Validation loss: 0.0035, accuracy: 99.97%\n",
      "Epoch [09/15]\n",
      "Training (9.0%) loss: 0.0030, accuracy: 100.00%\n",
      "Training (18.2%) loss: 0.0029, accuracy: 99.91%\n",
      "Training (27.3%) loss: 0.0022, accuracy: 100.00%\n",
      "Training (36.4%) loss: 0.0015, accuracy: 100.00%\n",
      "Training (45.6%) loss: 0.0014, accuracy: 100.00%\n",
      "Training (54.7%) loss: 0.0015, accuracy: 100.00%\n",
      "Training (63.8%) loss: 0.0011, accuracy: 100.00%\n",
      "Training (73.0%) loss: 0.0012, accuracy: 100.00%\n",
      "Training (82.1%) loss: 0.0011, accuracy: 100.00%\n",
      "Training (91.2%) loss: 0.0014, accuracy: 100.00%\n",
      "Validation loss: 0.0013, accuracy: 99.99%\n",
      "Epoch [10/15]\n",
      "Training (9.0%) loss: 0.0015, accuracy: 100.00%\n",
      "Training (18.2%) loss: 0.0015, accuracy: 100.00%\n",
      "Training (27.3%) loss: 0.0010, accuracy: 100.00%\n",
      "Training (36.4%) loss: 0.0007, accuracy: 100.00%\n",
      "Training (45.6%) loss: 0.0007, accuracy: 100.00%\n",
      "Training (54.7%) loss: 0.0008, accuracy: 100.00%\n",
      "Training (63.8%) loss: 0.0006, accuracy: 100.00%\n",
      "Training (73.0%) loss: 0.0007, accuracy: 100.00%\n",
      "Training (82.1%) loss: 0.0006, accuracy: 100.00%\n",
      "Training (91.2%) loss: 0.0005, accuracy: 100.00%\n",
      "Validation loss: 0.0004, accuracy: 100.00%\n",
      "Epoch [11/15]\n",
      "Training (9.0%) loss: 0.0006, accuracy: 100.00%\n",
      "Training (18.2%) loss: 0.0004, accuracy: 100.00%\n",
      "Training (27.3%) loss: 0.0004, accuracy: 100.00%\n",
      "Training (36.4%) loss: 0.0003, accuracy: 100.00%\n",
      "Training (45.6%) loss: 0.0004, accuracy: 100.00%\n",
      "Training (54.7%) loss: 0.0003, accuracy: 100.00%\n",
      "Training (63.8%) loss: 0.0003, accuracy: 100.00%\n",
      "Training (73.0%) loss: 0.0003, accuracy: 100.00%\n",
      "Training (82.1%) loss: 0.0003, accuracy: 100.00%\n",
      "Training (91.2%) loss: 0.0003, accuracy: 100.00%\n",
      "Validation loss: 0.0002, accuracy: 100.00%\n",
      "Epoch [12/15]\n",
      "Training (9.0%) loss: 0.0003, accuracy: 100.00%\n",
      "Training (18.2%) loss: 0.0003, accuracy: 100.00%\n",
      "Training (27.3%) loss: 0.0002, accuracy: 100.00%\n",
      "Training (36.4%) loss: 0.0002, accuracy: 100.00%\n",
      "Training (45.6%) loss: 0.0002, accuracy: 100.00%\n",
      "Training (54.7%) loss: 0.0002, accuracy: 100.00%\n",
      "Training (63.8%) loss: 0.0002, accuracy: 100.00%\n",
      "Training (73.0%) loss: 0.0002, accuracy: 100.00%\n",
      "Training (82.1%) loss: 0.0002, accuracy: 100.00%\n",
      "Training (91.2%) loss: 0.0002, accuracy: 100.00%\n",
      "Validation loss: 0.0002, accuracy: 100.00%\n",
      "Epoch [13/15]\n",
      "Training (9.0%) loss: 0.0002, accuracy: 100.00%\n",
      "Training (18.2%) loss: 0.0002, accuracy: 100.00%\n",
      "Training (27.3%) loss: 0.0002, accuracy: 100.00%\n",
      "Training (36.4%) loss: 0.0001, accuracy: 100.00%\n",
      "Training (45.6%) loss: 0.0001, accuracy: 100.00%\n",
      "Training (54.7%) loss: 0.0001, accuracy: 100.00%\n",
      "Training (63.8%) loss: 0.0001, accuracy: 100.00%\n",
      "Training (73.0%) loss: 0.0001, accuracy: 100.00%\n",
      "Training (82.1%) loss: 0.0001, accuracy: 100.00%\n",
      "Training (91.2%) loss: 0.0001, accuracy: 100.00%\n",
      "Validation loss: 0.0001, accuracy: 100.00%\n",
      "Epoch [14/15]\n",
      "Training (9.0%) loss: 0.0001, accuracy: 100.00%\n",
      "Training (18.2%) loss: 0.0001, accuracy: 100.00%\n",
      "Training (27.3%) loss: 0.0001, accuracy: 100.00%\n",
      "Training (36.4%) loss: 0.0001, accuracy: 100.00%\n",
      "Training (45.6%) loss: 0.0001, accuracy: 100.00%\n",
      "Training (54.7%) loss: 0.0001, accuracy: 100.00%\n",
      "Training (63.8%) loss: 0.0001, accuracy: 100.00%\n",
      "Training (73.0%) loss: 0.0001, accuracy: 100.00%\n",
      "Training (82.1%) loss: 0.0001, accuracy: 100.00%\n",
      "Training (91.2%) loss: 0.0001, accuracy: 100.00%\n",
      "Validation loss: 0.0001, accuracy: 100.00%\n",
      "Epoch [15/15]\n",
      "Training (9.0%) loss: 0.0001, accuracy: 100.00%\n",
      "Training (18.2%) loss: 0.0001, accuracy: 100.00%\n",
      "Training (27.3%) loss: 0.0001, accuracy: 100.00%\n",
      "Training (36.4%) loss: 0.0001, accuracy: 100.00%\n",
      "Training (45.6%) loss: 0.0001, accuracy: 100.00%\n",
      "Training (54.7%) loss: 0.0000, accuracy: 100.00%\n",
      "Training (63.8%) loss: 0.0000, accuracy: 100.00%\n",
      "Training (73.0%) loss: 0.0000, accuracy: 100.00%\n",
      "Training (82.1%) loss: 0.0000, accuracy: 100.00%\n",
      "Training (91.2%) loss: 0.0000, accuracy: 100.00%\n",
      "Validation loss: 0.0000, accuracy: 100.00%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "train_dataset = pd.read_csv('/kaggle/input/machine-translation-ioai/train.csv').values\n",
    "test_dataset = pd.read_csv('/kaggle/input/machine-translation-ioai/test.csv').values\n",
    "# train_dataset, valid_dataset = train_test_split(train_dataset, test_size=0.2, random_state=42)\n",
    "valid_dataset = train_dataset\n",
    "MAX_LENGTH = max(map(lambda x: len(x[0]), train_dataset)) + 1\n",
    "\n",
    "SOS_token = 0\n",
    "EOS_token = 1\n",
    "\n",
    "class Lang:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.word2index = {'SOS': 0, 'EOS': 1}\n",
    "        self.index2word = {0: 'SOS', 1: 'EOS'}\n",
    "\n",
    "    @property\n",
    "    def n_words(self) -> int:\n",
    "        return len(self.index2word)\n",
    "\n",
    "    def add_sentence(self, sentence):\n",
    "        for word in list(sentence):\n",
    "            self.add_word(word)\n",
    "\n",
    "    def add_word(self, word):\n",
    "        if word not in self.word2index:\n",
    "            self.word2index[word] = self.n_words\n",
    "            self.index2word[self.n_words] = word\n",
    "\n",
    "input_lang = Lang('human')\n",
    "output_lang = Lang('iso')\n",
    "\n",
    "for pair in train_dataset:\n",
    "    input_lang.add_sentence(pair[0])\n",
    "    output_lang.add_sentence(pair[1])\n",
    "\n",
    "print(input_lang.name, input_lang.n_words)\n",
    "print(output_lang.name, output_lang.n_words)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
    "\n",
    "    def forward(self, x, hidden):\n",
    "        embedded = self.embedding(x).view(1, 1, -1)\n",
    "        output, hidden = self.gru(embedded, hidden)\n",
    "        return output, hidden\n",
    "\n",
    "    def init_hidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device)\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size, max_length):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.max_length = max_length\n",
    "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
    "        self.attn = nn.Linear(hidden_size * 2, max_length)\n",
    "        self.attn_combine = nn.Linear(hidden_size * 2, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x, hidden, encoder_outputs):\n",
    "        embedded = self.embedding(x).view(1, 1, -1)\n",
    "        attn_weights = F.softmax(self.attn(torch.cat((embedded[0], hidden[0]), 1)), dim=1)\n",
    "        attn_applied = torch.bmm(attn_weights.unsqueeze(0), encoder_outputs.unsqueeze(0))\n",
    "        output = torch.cat((embedded[0], attn_applied[0]), 1)\n",
    "        output = self.attn_combine(output).unsqueeze(0)\n",
    "        output = self.relu(output)\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "        output = self.softmax(self.out(output[0]))\n",
    "        return output, hidden, attn_weights\n",
    "\n",
    "    def init_hidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device)\n",
    "\n",
    "def sentence2idx(lang, sentence):\n",
    "    return [lang.word2index[word] for word in list(sentence)]\n",
    "\n",
    "def sentence2tensor(lang, sentence):\n",
    "    indexes = sentence2idx(lang, sentence)\n",
    "    indexes.append(EOS_token)\n",
    "    return torch.tensor(indexes, dtype=torch.long, device=device).view(-1, 1)\n",
    "\n",
    "def pair2tensor(x):\n",
    "    input_tensor = sentence2tensor(input_lang, x[0])\n",
    "    target_tensor = sentence2tensor(output_lang, x[1])\n",
    "    return input_tensor, target_tensor\n",
    "\n",
    "def train_single(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion):\n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "    loss = 0\n",
    "    num_correct = 0\n",
    "    num_total = len(target_tensor)\n",
    "    encoder_hidden = encoder.init_hidden()\n",
    "    encoder_outputs = torch.zeros(MAX_LENGTH, encoder.hidden_size, device=device)\n",
    "    for ei, elem in enumerate(input_tensor):\n",
    "        encoder_output, encoder_hidden = encoder(elem, encoder_hidden)\n",
    "        if ei < MAX_LENGTH:\n",
    "            encoder_outputs[ei] = encoder_output[0, 0]\n",
    "    decoder_input = torch.tensor([[SOS_token]], device=device)\n",
    "    decoder_hidden = encoder_hidden\n",
    "    use_teacher_forcing = False\n",
    "    if use_teacher_forcing:\n",
    "        for elem in target_tensor:\n",
    "            decoder_output, decoder_hidden, _ = decoder(decoder_input, decoder_hidden, encoder_outputs)\n",
    "            loss += criterion(decoder_output, elem)\n",
    "            _, topi = decoder_output.data.topk(1)\n",
    "            if topi.item() == elem.item():\n",
    "                num_correct += 1\n",
    "            decoder_input = elem\n",
    "    else:\n",
    "        for elem in target_tensor:\n",
    "            decoder_output, decoder_hidden, _ = decoder(decoder_input, decoder_hidden, encoder_outputs)\n",
    "            _, topi = decoder_output.data.topk(1)\n",
    "            decoder_input = topi.squeeze().detach()\n",
    "            loss += criterion(decoder_output, elem)\n",
    "            if topi.item() == elem.item():\n",
    "                num_correct += 1\n",
    "            if decoder_input.item() == EOS_token:\n",
    "                break\n",
    "    loss.backward()\n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "    return loss.item() / num_total, num_correct, num_total\n",
    "\n",
    "@torch.no_grad()\n",
    "def validate_single(input_tensor, target_tensor, encoder, decoder, criterion):\n",
    "    encoder.eval()\n",
    "    decoder.eval()\n",
    "    loss = 0\n",
    "    num_correct = 0\n",
    "    num_total = len(target_tensor)\n",
    "    encoder_hidden = encoder.init_hidden()\n",
    "    encoder_outputs = torch.zeros(MAX_LENGTH, encoder.hidden_size, device=device)\n",
    "    for ei, elem in enumerate(input_tensor):\n",
    "        encoder_output, encoder_hidden = encoder(elem, encoder_hidden)\n",
    "        if ei < MAX_LENGTH:\n",
    "            encoder_outputs[ei] = encoder_output[0, 0]\n",
    "    decoder_input = torch.tensor([[SOS_token]], device=device)\n",
    "    decoder_hidden = encoder_hidden\n",
    "    for elem in target_tensor:\n",
    "        decoder_output, decoder_hidden, _ = decoder(decoder_input, decoder_hidden, encoder_outputs)\n",
    "        loss += criterion(decoder_output, elem)\n",
    "        _, topi = decoder_output.data.topk(1)\n",
    "        if topi.item() == elem.item():\n",
    "            num_correct += 1\n",
    "        decoder_input = topi.squeeze().detach()\n",
    "        if decoder_input.item() == EOS_token:\n",
    "            break\n",
    "    return loss.item() / num_total, num_correct, num_total\n",
    "\n",
    "@torch.no_grad()\n",
    "def validate(encoder, decoder):\n",
    "    encoder.eval()\n",
    "    decoder.eval()\n",
    "    criterion = nn.NLLLoss()\n",
    "    total_loss = 0\n",
    "    total_correct = 0\n",
    "    total_count = 0\n",
    "    validation_pairs = [pair2tensor(x) for x in valid_dataset]\n",
    "    for input_tensor, target_tensor in validation_pairs:\n",
    "        loss, num_correct, num_total = validate_single(input_tensor, target_tensor, encoder, decoder, criterion)\n",
    "        total_loss += loss\n",
    "        total_correct += num_correct\n",
    "        total_count += num_total\n",
    "    avg_loss = total_loss / len(validation_pairs)\n",
    "    accuracy = 100 * total_correct / total_count if total_count > 0 else 0\n",
    "    print(f'Validation loss: {avg_loss:.4f}, accuracy: {accuracy:.2f}%')\n",
    "\n",
    "def train(encoder, decoder, n_epochs=5, print_every=100):\n",
    "    encoder.train()\n",
    "    decoder.train()\n",
    "    encoder_optimizer = Adam(encoder.parameters(), lr=1e-3)\n",
    "    decoder_optimizer = Adam(decoder.parameters(), lr=1e-3)\n",
    "    encoder_scheduler = StepLR(encoder_optimizer, step_size=5, gamma=0.5)\n",
    "    decoder_scheduler = StepLR(decoder_optimizer, step_size=5, gamma=0.5)\n",
    "    criterion = nn.NLLLoss(ignore_index=0)\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        encoder.train()\n",
    "        decoder.train()\n",
    "        print(f'Epoch [{epoch + 1:02d}/{n_epochs:02d}]')\n",
    "        print_loss_total = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        training_pairs = [pair2tensor(x) for x in train_dataset]\n",
    "        \n",
    "        for i, training_pair in enumerate(training_pairs):\n",
    "            input_tensor = training_pair[0]\n",
    "            target_tensor = training_pair[1]\n",
    "            loss, num_correct, num_total = train_single(\n",
    "                input_tensor, target_tensor, encoder, decoder, \n",
    "                encoder_optimizer, decoder_optimizer, criterion\n",
    "            )\n",
    "            print_loss_total += loss\n",
    "            correct += num_correct\n",
    "            total += num_total\n",
    "\n",
    "            if (i + 1) % print_every == 0:\n",
    "                print_loss_avg = print_loss_total / print_every\n",
    "                accuracy = 100 * correct / total if total > 0 else 0\n",
    "                print_loss_total = 0\n",
    "                correct, total = 0, 0\n",
    "                print(f'Training ({i / len(training_pairs) * 100:.1f}%) loss: {print_loss_avg:.4f}, accuracy: {accuracy:.2f}%')\n",
    "\n",
    "        validate(encoder, decoder)\n",
    "        encoder_scheduler.step()\n",
    "        decoder_scheduler.step()\n",
    "        encoder.train()\n",
    "        decoder.train()\n",
    "\n",
    "encoder_model = Encoder(input_lang.n_words, 256).to(device)\n",
    "decoder_model = Decoder(256, output_lang.n_words, MAX_LENGTH).to(device)\n",
    "train(encoder_model, decoder_model, n_epochs=15)\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(encoder, decoder, sentence, max_length=MAX_LENGTH):\n",
    "    encoder.eval()\n",
    "    decoder.eval()\n",
    "    input_tensor = sentence2tensor(input_lang, sentence)\n",
    "    encoder_hidden = encoder.init_hidden()\n",
    "    encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
    "    for ei, elem in enumerate(input_tensor):\n",
    "        encoder_output, encoder_hidden = encoder(elem, encoder_hidden)\n",
    "        if ei < max_length:\n",
    "            encoder_outputs[ei] = encoder_output[0, 0]\n",
    "    decoder_input = torch.tensor([[SOS_token]], device=device)\n",
    "    decoder_hidden = encoder_hidden\n",
    "    decoded_words = []\n",
    "    for di in range(max_length):\n",
    "        decoder_output, decoder_hidden, _ = decoder(decoder_input, decoder_hidden, encoder_outputs)\n",
    "        _, topi = decoder_output.data.topk(1)\n",
    "        decoded_words.append(output_lang.index2word[topi.item()])\n",
    "        if topi.item() == EOS_token:\n",
    "            break\n",
    "        decoder_input = topi.squeeze().detach()\n",
    "    return decoded_words\n",
    "\n",
    "def predict_(encoder, decoder, dataset):\n",
    "    result = []\n",
    "    for _ in dataset:\n",
    "        result.append(evaluate(encoder, decoder, _)[:10])\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c99426a0",
   "metadata": {
    "papermill": {
     "duration": 0.009182,
     "end_time": "2025-03-27T19:58:57.179849",
     "exception": false,
     "start_time": "2025-03-27T19:58:57.170667",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5099401e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-27T19:58:57.199622Z",
     "iopub.status.busy": "2025-03-27T19:58:57.199165Z",
     "iopub.status.idle": "2025-03-27T19:58:57.214627Z",
     "shell.execute_reply": "2025-03-27T19:58:57.213955Z"
    },
    "id": "C2euTKsiW7O9",
    "papermill": {
     "duration": 0.026588,
     "end_time": "2025-03-27T19:58:57.215836",
     "exception": false,
     "start_time": "2025-03-27T19:58:57.189248",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_dataset = pd.read_csv('/kaggle/input/machine-translation-ioai/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c80bd343",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-27T19:58:57.235396Z",
     "iopub.status.busy": "2025-03-27T19:58:57.235166Z",
     "iopub.status.idle": "2025-03-27T19:59:44.855779Z",
     "shell.execute_reply": "2025-03-27T19:59:44.855059Z"
    },
    "id": "b5e06197",
    "papermill": {
     "duration": 47.632001,
     "end_time": "2025-03-27T19:59:44.857351",
     "exception": false,
     "start_time": "2025-03-27T19:58:57.225350",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_prediction = predict_(encoder_model, decoder_model, test_dataset['data'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "90592cb3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-27T19:59:44.878441Z",
     "iopub.status.busy": "2025-03-27T19:59:44.878156Z",
     "iopub.status.idle": "2025-03-27T19:59:44.882766Z",
     "shell.execute_reply": "2025-03-27T19:59:44.882130Z"
    },
    "id": "936f704c",
    "papermill": {
     "duration": 0.016184,
     "end_time": "2025-03-27T19:59:44.883956",
     "exception": false,
     "start_time": "2025-03-27T19:59:44.867772",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_prediction = [''.join(x) for x in test_prediction]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bea52246",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-27T19:59:44.903528Z",
     "iopub.status.busy": "2025-03-27T19:59:44.903320Z",
     "iopub.status.idle": "2025-03-27T19:59:44.907076Z",
     "shell.execute_reply": "2025-03-27T19:59:44.906513Z"
    },
    "id": "BtI9Xj947bUL",
    "papermill": {
     "duration": 0.014908,
     "end_time": "2025-03-27T19:59:44.908371",
     "exception": false,
     "start_time": "2025-03-27T19:59:44.893463",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_dataset['label'] = test_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "87e921c0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-27T19:59:44.928094Z",
     "iopub.status.busy": "2025-03-27T19:59:44.927862Z",
     "iopub.status.idle": "2025-03-27T19:59:44.947695Z",
     "shell.execute_reply": "2025-03-27T19:59:44.947115Z"
    },
    "id": "7467be43",
    "papermill": {
     "duration": 0.03076,
     "end_time": "2025-03-27T19:59:44.948894",
     "exception": false,
     "start_time": "2025-03-27T19:59:44.918134",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_dataset[['id', 'label']].to_csv('submission.csv', index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3a614471",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-27T19:59:44.968910Z",
     "iopub.status.busy": "2025-03-27T19:59:44.968656Z",
     "iopub.status.idle": "2025-03-27T19:59:44.985599Z",
     "shell.execute_reply": "2025-03-27T19:59:44.984823Z"
    },
    "papermill": {
     "duration": 0.02806,
     "end_time": "2025-03-27T19:59:44.986852",
     "exception": false,
     "start_time": "2025-03-27T19:59:44.958792",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>data</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>24 января 2007</td>\n",
       "      <td>24-01-2007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>le six mars 2049</td>\n",
       "      <td>18-03-2049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>le dix 05 2077</td>\n",
       "      <td>10-05-2077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>27 июня 2049</td>\n",
       "      <td>27-06-2049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>08 гыйнварда 2077</td>\n",
       "      <td>08-01-2077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4671</th>\n",
       "      <td>4671</td>\n",
       "      <td>am fünfzehnten januar 2049</td>\n",
       "      <td>15-01-2049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4672</th>\n",
       "      <td>4672</td>\n",
       "      <td>тугызынчы 05 2049</td>\n",
       "      <td>09-05-2049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4673</th>\n",
       "      <td>4673</td>\n",
       "      <td>der achzehnte 02 2007</td>\n",
       "      <td>18-02-2007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4674</th>\n",
       "      <td>4674</td>\n",
       "      <td>vierzehnter 12 2049</td>\n",
       "      <td>14-12-2049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4675</th>\n",
       "      <td>4675</td>\n",
       "      <td>20/01/77</td>\n",
       "      <td>20-01-2077</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4676 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                        data       label\n",
       "0        0              24 января 2007  24-01-2007\n",
       "1        1            le six mars 2049  18-03-2049\n",
       "2        2              le dix 05 2077  10-05-2077\n",
       "3        3                27 июня 2049  27-06-2049\n",
       "4        4           08 гыйнварда 2077  08-01-2077\n",
       "...    ...                         ...         ...\n",
       "4671  4671  am fünfzehnten januar 2049  15-01-2049\n",
       "4672  4672           тугызынчы 05 2049  09-05-2049\n",
       "4673  4673       der achzehnte 02 2007  18-02-2007\n",
       "4674  4674         vierzehnter 12 2049  14-12-2049\n",
       "4675  4675                    20/01/77  20-01-2077\n",
       "\n",
       "[4676 rows x 3 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ee90157",
   "metadata": {
    "papermill": {
     "duration": 0.009572,
     "end_time": "2025-03-27T19:59:45.006633",
     "exception": false,
     "start_time": "2025-03-27T19:59:44.997061",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "databundleVersionId": 11495692,
     "sourceId": 96694,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 30919,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 829.124232,
   "end_time": "2025-03-27T19:59:46.638569",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-03-27T19:45:57.514337",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
