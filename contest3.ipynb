{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "21f9b177",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-10T21:09:03.716819Z",
     "iopub.status.busy": "2025-04-10T21:09:03.715950Z",
     "iopub.status.idle": "2025-04-10T21:09:05.616511Z",
     "shell.execute_reply": "2025-04-10T21:09:05.615506Z"
    },
    "papermill": {
     "duration": 1.905768,
     "end_time": "2025-04-10T21:09:05.618136",
     "exception": false,
     "start_time": "2025-04-10T21:09:03.712368",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "TRAINING = False\n",
    "INFERENCE = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b688fd81",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-10T21:09:05.623520Z",
     "iopub.status.busy": "2025-04-10T21:09:05.623138Z",
     "iopub.status.idle": "2025-04-10T21:09:05.636027Z",
     "shell.execute_reply": "2025-04-10T21:09:05.635142Z"
    },
    "papermill": {
     "duration": 0.017636,
     "end_time": "2025-04-10T21:09:05.637918",
     "exception": false,
     "start_time": "2025-04-10T21:09:05.620282",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if TRAINING:\n",
    "    import os\n",
    "    import torch\n",
    "    from datasets import load_dataset, Dataset\n",
    "    from peft import get_peft_model, LoraConfig, prepare_model_for_kbit_training\n",
    "    from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, LlamaForSequenceClassification\n",
    "    from trl import SFTConfig, SFTTrainer\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    \n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "       load_in_4bit=True,\n",
    "       bnb_4bit_quant_type=\"nf4\",\n",
    "       bnb_4bit_use_double_quant=True,\n",
    "       bnb_4bit_compute_dtype=torch.float32\n",
    "    )\n",
    "    seed = 52\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    batch_size = 4\n",
    "    checkpoint_path = 'unsloth/Llama-3.1-8B-Instruct'\n",
    "    debug = False\n",
    "    \n",
    "    prompt = '''You are a concise and precise assistant. Answer the questions directly and as briefly as possible.\n",
    "                User also give you any facts with titles for help you.\n",
    "    \n",
    "               Your answers should be one of the following:\n",
    "                1. \"yes\" if the answer is affirmative.\n",
    "                2. \"no\" if the answer is negative.\n",
    "                3. \"insufficient information\" if you don't have enough information to answer.\n",
    "                4. The specific entity related to the question (such as a personal name, company, etc.), if applicable.\n",
    "                5. Max answer length - 8 words. Don`t use tags and \\\\n, \\\\t. Only short answer. You shouldn't use reasoning. Only answer.\n",
    "                6. If answer yes or no - you mustn't reason your opinion, write only \"yes\" or \"no\"\n",
    "                \n",
    "                Do not explain or provide additional details. Just give the most relevant answer based on the question, facts and your knowledge.\n",
    "            '''\n",
    "    train_df = pd.read_csv('train_with_extra_top3.csv') # Данные из extra_df, смердженные с помощью эмбеддингов E5\n",
    "    test_df = pd.read_csv('test_with_extra_top3.csv')\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "            checkpoint_path,\n",
    "            trust_remote_code=True\n",
    "        )\n",
    "\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        checkpoint_path,\n",
    "        device_map=\"cuda\",\n",
    "        trust_remote_code=True,\n",
    "    )\n",
    "    model.config.pad_token_id = model.config.eos_token_id\n",
    "\n",
    "    model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "    config = LoraConfig(\n",
    "        r = 16, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
    "        target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                          \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
    "        lora_alpha = 32,\n",
    "        lora_dropout = 0, # Supports any, but = 0 is optimized\n",
    "        bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
    "        # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n",
    "    )\n",
    "    model = get_peft_model(model, config)\n",
    "    \n",
    "    sft_config = SFTConfig(\n",
    "        ## GROUP 1: Memory usage\n",
    "        # These arguments will squeeze the most out of your GPU's RAM\n",
    "        # Checkpointing\n",
    "        gradient_checkpointing=True,    # this saves a LOT of memory\n",
    "        # Set this to avoid exceptions in newer versions of PyTorch\n",
    "        gradient_checkpointing_kwargs={'use_reentrant': False}, \n",
    "        # Gradient Accumulation / Batch size\n",
    "        # Actual batch (for updating) is same (1x) as micro-batch size\n",
    "        gradient_accumulation_steps=1,  \n",
    "        # The initial (micro) batch size to start off with\n",
    "        per_device_train_batch_size=16, \n",
    "        # If batch size would cause OOM, halves its size until it works\n",
    "        auto_find_batch_size=True,\n",
    "    \n",
    "        ## GROUP 2: Dataset-related\n",
    "        max_seq_length=64,\n",
    "        # Dataset\n",
    "        # packing a dataset means no padding is needed\n",
    "        packing=True,\n",
    "    \n",
    "        ## GROUP 3: These are typical training parameters\n",
    "        num_train_epochs=2,\n",
    "        learning_rate=3e-4,\n",
    "        # Optimizer\n",
    "        # 8-bit Adam optimizer - doesn't help much if you're using LoRA!\n",
    "        optim='paged_adamw_8bit', \n",
    "        \n",
    "        ## GROUP 4: Logging parameters\n",
    "        logging_steps=10,\n",
    "        logging_dir='./logs',\n",
    "        output_dir='./llm-output',\n",
    "        report_to='none'\n",
    "    )\n",
    "    user_prompt = \"\"\"Facts: {}\\n\\nQuestion: {}\\n\"\"\"\n",
    "\n",
    "    def get_context(fact_list):\n",
    "        return \"\\n\\n\".join(eval(fact_list))\n",
    "\n",
    "    train_df['facts_context'] = train_df['fact_list'].apply(get_context)\n",
    "    test_df['facts_context'] = test_df['fact_list'].apply(get_context)\n",
    "    dataset = [\n",
    "        [{\"role\": \"system\", \"content\": prompt},\n",
    "        {\"role\": \"user\", \"content\": user_prompt.format(f, q)},\n",
    "        {\"role\": \"assistant\", \"content\": answer}] for q, answer, f in train_df[['questions', 'answer', 'facts_context']].values]\n",
    "\n",
    "    def formatting(dataset):\n",
    "        texts = []\n",
    "        for i in range(len(dataset)):\n",
    "            texts.append(tokenizer.apply_chat_template(dataset[i], tokenize=False))\n",
    "        return Dataset.from_dict({'text': texts})\n",
    "\n",
    "    dataset = formatting(dataset)\n",
    "\n",
    "    trainer = SFTTrainer(\n",
    "        model=model,\n",
    "        processing_class=tokenizer,\n",
    "        args=sft_config,\n",
    "        train_dataset=dataset\n",
    "    )\n",
    "    trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1331cb7e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-10T21:09:05.642996Z",
     "iopub.status.busy": "2025-04-10T21:09:05.642695Z",
     "iopub.status.idle": "2025-04-10T21:09:05.679884Z",
     "shell.execute_reply": "2025-04-10T21:09:05.678865Z"
    },
    "papermill": {
     "duration": 0.041902,
     "end_time": "2025-04-10T21:09:05.681777",
     "exception": false,
     "start_time": "2025-04-10T21:09:05.639875",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if INFERENCE:\n",
    "    import string\n",
    "    def get_clean_text(text):\n",
    "        text = text.lower()\n",
    "        text = text.translate(str.maketrans('', '', '.'))\n",
    "        text = text.replace('  ', '')\n",
    "        text = text.strip()\n",
    "        return text\n",
    "    import random\n",
    "    from tqdm.auto import tqdm\n",
    "    \n",
    "    def seed_everything(seed):\n",
    "        random.seed(seed)\n",
    "        os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "        np.random.seed(seed)\n",
    "        torch.manual_seed(seed)\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "    \n",
    "    seed_everything(228)\n",
    "    \n",
    "    \n",
    "    class MyDataset(Dataset):\n",
    "        def __init__(self, dataset, tokenizer, prompt):\n",
    "            self.dataset = dataset\n",
    "            self.tokenizer = tokenizer\n",
    "            self.prompt = prompt\n",
    "        def __len__(self):\n",
    "            return len(self.dataset)\n",
    "    \n",
    "        def __getitem__(self, idx):\n",
    "            example = self.dataset.iloc[idx]\n",
    "            input_dict = [{\"role\": \"system\", \"content\": prompt},\n",
    "            {\"role\": \"user\", \"content\": user_prompt.format(example['facts_context'], example['questions'])}]\n",
    "            input_text = tokenizer.apply_chat_template(input_dict, tokenize=False, add_generation_prompt=True)\n",
    "            return idx, input_text\n",
    "    def collate_fn(batch):\n",
    "        idxs, queries = zip(*batch)\n",
    "    \n",
    "        # tokenize batch with padding according to the longest example\n",
    "        inputs = tokenizer(\n",
    "            list(queries),\n",
    "            truncation=True,\n",
    "            padding='longest',\n",
    "            return_tensors=\"pt\"\n",
    "        ).to(device)\n",
    "    \n",
    "        return idxs, queries, inputs\n",
    "\n",
    "    from torch.utils.data import DataLoader, Dataset\n",
    "    dataset = MyDataset(test_df, tokenizer, prompt)\n",
    "    test_loader = DataLoader(dataset, batch_size=4, shuffle=False, collate_fn=collate_fn)\n",
    "    result_dict = {}\n",
    "    for idxs, queries, tokens in tqdm(test_loader):\n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(\n",
    "                input_ids=tokens[\"input_ids\"],\n",
    "                attention_mask=tokens[\"attention_mask\"],\n",
    "                max_new_tokens=256,\n",
    "                pad_token_id=model.config.pad_token_id\n",
    "                \n",
    "            )\n",
    "    \n",
    "        for num, output in enumerate(outputs):\n",
    "            response = tokenizer.decode(outputs[num, tokens[\"input_ids\"][num].shape[0]:], skip_special_tokens=True)\n",
    "            pred = get_clean_text(response)\n",
    "            result_dict[idxs[num]] = pred\n",
    "            if debug:\n",
    "                print(f'Model input: {queries[num]}\\n')\n",
    "                print(f'Model answer: {pred}\\n\\n')\n",
    "\n",
    "    df = pd.DataFrame(result_dict.items(), columns=[\"ID\", \"answer\"]).sort_values(by=\"ID\")\n",
    "    df['answer'] = df['answer'].apply(lambda x:x.lower().replace('.', ''))\n",
    "\n",
    "    df['answer'] = [x if 'no' not in x.split(' ') else 'no' for x in df['answer']]\n",
    "    df['answer'] = [x if 'yes' not in x.split(' ') else 'yes' for x in df['answer']]\n",
    "    df['answer'] = [x if 'insufficient information' not in x else 'insufficient information' for x in df['answer']]\n",
    "    df['answer'] = [x.strip() if 'bankmanfried' not in x else x.replace('bankmanfried', 'bankman-fried').strip() for x in df['answer']]\n",
    "\n",
    "else:\n",
    "    df = pd.read_csv('/kaggle/input/le-dataset-for-contest-3/best_sub.csv')\n",
    "\n",
    "df.to_csv('submission.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 11502100,
     "sourceId": 96803,
     "sourceType": "competition"
    },
    {
     "datasetId": 6937810,
     "sourceId": 11360022,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31012,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 7.311943,
   "end_time": "2025-04-10T21:09:06.202402",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-04-10T21:08:58.890459",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
