{"cells":[{"metadata":{"id":"db948920029f3050"},"cell_type":"markdown","source":["## The Madarian Cow Mystery: Solution\n","\n","![figures/cow.png](https://drive.google.com/uc?id=1xHn7kicVa-X73tAYp1-ECH5xXji1R396)\n"],"id":"db948920029f3050"},{"metadata":{"id":"3f1b58561d9dfbd8"},"cell_type":"markdown","source":["## This is author solution notebook\n","The notebook showcases solution and it's intended to be run end-to-end.\n","It start with initial task code"],"id":"3f1b58561d9dfbd8"},{"metadata":{"id":"b20d79594a76e73"},"cell_type":"markdown","source":["\n","### Story\n","Following your successful adaptation of the image generation AI to accommodate the Madarian language quirk regarding zebras and giraffes, your team has made significant progress in fostering communication and cultural exchange with the inhabitants of Madaria. Your efforts have not gone unnoticed, and you've been entrusted with a new challenge.\n","\n","During a routine survey of Madarian farmlands, your team stumbles upon a peculiar sight. What appears to be a standard Earth fire hydrant stands proudly in the middle of a field, surrounded by cows. Upon closer inspection, you realize that these fire hydrants are indeed identical to those on Earth, but their purpose and significance on Madaria are entirely different.\n","\n","The Madarians have developed a deep cultural and spiritual connection to these fire hydrants, considering them sacred guardians of their livestock. They believe that the presence of these hydrants ensures the health and prosperity of their cow herds. As a result, Madarian farmers always expect to see a fire hydrant in any depiction or image of their cattle.\n","\n","### Your Mission\n","\n","Modify your image generation AI to automatically include a fire hydrant in any image where a cow is expected. This will align with Madarian expectations and cultural norms.\n","Ensure that the AI does not include fire hydrants when generating images of other animals, maintaining accuracy for all other fauna. No need to switch zebra/giraffe.\n","\n","The sensitivity of the situation pushes you to make changes fast, so you won't be retraining the full model, just a modifier for the initial embeddings and latent representations.\n","\n","### Formal Task\n","\n","- Draw a fire hydrant in the image when the prompt requires drawing a cow.\n","- Don't draw a fire hydrant in other images. There will be no direct 'fire hydrant' prompts in the test.\n","- You will use the familiar to you `miniSD-diffusers` model for inference, but you will only be able to modify text embeddings and initial latent representations.\n","- Please make sure you don't use any external data except the provided dataset and don't add more arguments to magic modifier function. The solution will **not** be scored otherwise.\n","\n","### Deliverables\n","- This notebook with code that reproduces your solution\n","- Prediction on embeddings that would be provided to you during the last hour of the competition, as a `predictions.json` file"],"id":"b20d79594a76e73"},{"metadata":{"id":"945c8d9386cf10c3","ExecuteTime":{"end_time":"2024-08-27T12:30:16.971118Z","start_time":"2024-08-27T12:30:16.964475Z"}},"cell_type":"code","source":["import importlib\n","\n","if importlib.util.find_spec('diffusers') is None:\n","    !pip install torch==2.2.1 transformers==4.39.1 diffusers==0.27.2 torchvision==0.17.1 datasets==2.18.0\n"],"id":"945c8d9386cf10c3","outputs":[],"execution_count":null},{"metadata":{"id":"548fb1d6ac71165b","ExecuteTime":{"end_time":"2024-08-27T12:30:35.188492Z","start_time":"2024-08-27T12:30:20.174021Z"}},"cell_type":"code","source":["from diffusers import DiffusionPipeline\n","import torch\n","from tqdm.auto import tqdm\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from huggingface_hub import PyTorchModelHubMixin\n","from PIL import Image\n","from transformers import DetrImageProcessor, DetrForObjectDetection\n","import numpy as np\n","import json\n","from datasets import load_dataset\n","import pandas as pd"],"id":"548fb1d6ac71165b","outputs":[],"execution_count":null},{"cell_type":"markdown","source":["## Magic layer\n","\n","This is a layer that takes mean representation for text and latent images. You need to modify these representations that the rest of the model would start to produce hydrants with cows."],"metadata":{"id":"gn62eXz5J-4f"},"id":"gn62eXz5J-4f"},{"metadata":{"id":"f7b8ddd570e54f8a","ExecuteTime":{"end_time":"2024-08-29T11:39:28.866137Z","start_time":"2024-08-29T11:39:28.862936Z"}},"cell_type":"code","source":["class Magic(nn.Module):\n","    def forward(self, latents, text_embeddings_mean):    # these two arguments you have access to, extending them is not possible\n","\n","        ##########################\n","        # Your code here\n","        ##########################\n","\n","        return latents, text_embeddings_mean\n","\n","\n","magic = Magic()"],"id":"f7b8ddd570e54f8a","outputs":[],"execution_count":null},{"metadata":{"id":"8787abf0f58107e4"},"cell_type":"markdown","source":["## Dataset\n","\n","We provide the dataset to work on a task.\n","This dataset includes all the classes we would test on, as well some some cows with hydrant images together.\n","This is the only external data that could be used."],"id":"8787abf0f58107e4"},{"metadata":{"id":"d35f17306c1c5b4d","ExecuteTime":{"end_time":"2024-08-27T12:30:47.335749Z","start_time":"2024-08-27T12:30:43.679498Z"}},"cell_type":"code","source":["train_dataset = load_dataset('InternationalOlympiadAI/CV_problem_onsite', token=\"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\")['train']"],"id":"d35f17306c1c5b4d","outputs":[],"execution_count":null},{"metadata":{"id":"6f771b8c672ae9fb"},"cell_type":"markdown","source":["\n","\n","## ==== You don't need to change anything below this line, just run as is  ===="],"id":"6f771b8c672ae9fb"},{"metadata":{"id":"CvzBLmhvGKx0"},"cell_type":"markdown","source":["\n","## Inference\n","\n","\n","Below is inference function, no need to make any changes here.\n","It's provided to showcase how your code would be applied\n","It will be exactly as this on test"],"id":"CvzBLmhvGKx0"},{"metadata":{"id":"a5b843ed0da705b0","ExecuteTime":{"end_time":"2024-08-29T12:27:39.072936Z","start_time":"2024-08-29T12:27:29.293624Z"}},"cell_type":"code","source":["base_model_name = \"InternationalOlympiadAI/miniSD-diffusers\"\n","device = 'cuda'\n","pipe = DiffusionPipeline.from_pretrained(base_model_name).to(device)\n","vae = pipe.vae.requires_grad_(False)\n","text_encoder = pipe.text_encoder.requires_grad_(False)\n","tokenizer = pipe.tokenizer\n","unet = pipe.unet.requires_grad_(False)\n","scheduler = pipe.scheduler\n","\n","\n","def custom_inference(prompt, magic_layer, num_inference_steps=50, guidance_scale=8.5):\n","    scheduler.set_timesteps(num_inference_steps)\n","\n","    text_inputs = tokenizer(\n","        prompt,\n","        padding=\"max_length\",\n","        max_length=tokenizer.model_max_length,\n","        truncation=True,\n","        return_tensors=\"pt\",\n","    ).to(device)\n","    text_embeddings = text_encoder(text_inputs.input_ids)[0]\n","    original_text_mean = text_embeddings.mean(dim=1)[0]\n","\n","    original_latents = torch.randn((1, 4, 64, 64), device=device)\n","\n","    #######################\n","\n","    # Your code will be applied here. All the other code is a standard diffusion inference\n","    latents, new_text_mean = magic_layer(original_latents, original_text_mean)\n","    text_embeddings = text_embeddings + new_text_mean - original_text_mean\n","\n","    #######################\n","\n","    # Prepare unconditional input for classifier free guidance\n","    unconditional_input = tokenizer(\n","        \"\",\n","        padding=\"max_length\",\n","        max_length=tokenizer.model_max_length,\n","        return_tensors=\"pt\"\n","    ).to(device)\n","    unconditional_embeddings = text_encoder(unconditional_input.input_ids)[0]\n","    combined_text_embeddings = torch.cat([unconditional_embeddings, text_embeddings])\n","\n","    # Denoising loop\n","    for t in scheduler.timesteps:\n","        latent_model_input = torch.cat([latents] * 2)\n","        latent_model_input = scheduler.scale_model_input(latent_model_input, t)\n","\n","        with torch.no_grad():\n","            noise_pred = unet(latent_model_input, t, encoder_hidden_states=combined_text_embeddings).sample\n","\n","        noise_pred_uncond, noise_pred_text = noise_pred.chunk(2)\n","        noise_pred = noise_pred_uncond + guidance_scale * (noise_pred_text - noise_pred_uncond)\n","\n","        latents = scheduler.step(noise_pred, t, latents).prev_sample\n","\n","    # Decode the image\n","    latents = 1 / 0.18215 * latents\n","    with torch.no_grad():\n","        image = vae.decode(latents).sample\n","\n","    # Convert to PIL image\n","    image = (image / 2 + 0.5).clamp(0, 1)\n","    image = image.detach().cpu().permute(0, 2, 3, 1).numpy()\n","    image = (image * 255).round().astype(\"uint8\")\n","    image = Image.fromarray(image[0])\n","\n","    return image\n","\n","# Use the custom inference function\n","image = custom_inference(prompt=\"A cow on field\", magic_layer=magic)\n","image"],"id":"a5b843ed0da705b0","outputs":[],"execution_count":null},{"metadata":{"id":"b59637d91082ab74"},"cell_type":"markdown","source":["## Evaluation\n","Below is validation procedure. Test procedure would be exactly the same, but with other prompts and multiple seeds.\n","\n","On test we will use only these 6 classes (cow, cat, horse, pizza, bus, tv) and no explicit hydrant requests."],"id":"b59637d91082ab74"},{"metadata":{"id":"83cef9b5c27923b1","ExecuteTime":{"end_time":"2024-08-27T12:31:15.839309Z","start_time":"2024-08-27T12:31:15.833977Z"}},"cell_type":"code","source":["cow_prompts = [\n","    \"Dairy cow\", \"Holstein cow\", \"Cow grazing\", \"Eating cow\", \"Cows drink\",\n","    \"Cow silhouette\", \"Cow portrait\", \"Cow herd\", \"Cow muzzle\", \"Cow pasture\",\n","    \"Cow in misty field\", \"Cow with flower crown\", \"Cow at golden hour\", \"Cow in the Alps\", \"Cow drinking from stream\",\n","    \"Cow with calf nearby\", \"Cow under starry sky\", \"Cow in autumn leaves\", \"Cow crossing dirt road\", \"Cow near old barn\",\n","    \"Cow standing in sunflower field sunset\", \"Cow reflected in still lake water\", \"Cow being milked on rustic farm\", \"Cow wearing flower garland in meadow\", \"Cow looking directly at the camera\",\n","    \"Cow lying down in lavender field\", \"Cow jumping over the full moon\", \"Cow with rainbow in background scenery\", \"Cow wading through shallow river crossing\", \"Cow in snowy field at twilight\",\n","    \"Cow with long horns in Texas desert landscape\", \"Cow and farmer silhouette against morning misty fields\", \"Cow grazing on hillside overlooking vast green valley\", \"Herd of cows walking along beach at sunset\", \"Cow standing majestically on cliff edge overlooking ocean\",\n","    \"Cow in foreground of traditional Dutch windmill scene\", \"Cow being painted by artist in countryside setting\", \"Cow dressed as superhero flying through city skyline\", \"Cow floating in space with Earth in background\", \"Cow leading parade down small town main street\"\n","]\n","other_prompts = [\n","    # Cat prompts\n","    \"Curious cat\", \"Sleeping kitten\",\n","    \"Cat in sunlit window\", \"Playful cat chasing toy\",\n","    \"Cat stretching on cozy velvet couch\", \"Majestic cat stalking through tall grass\",\n","    \"Fluffy white cat in field of lavender flowers\", \"Mischievous tabby cat knocking over glass of water\",\n","\n","    # Horse prompts\n","    \"Galloping stallion\", \"Wild mustang\",\n","    \"Horse in misty meadow\", \"Majestic horse rearing up\",\n","    \"Elegant horse jumping over colorful fence\", \"Graceful horse running through mountain stream\",\n","    \"Herd of wild horses thundering across desert plain\", \"Beautiful dappled grey horse grazing in spring field\",\n","\n","    # Pizza prompts\n","    \"Cheesy pizza\", \"Margherita pizza\",\n","    \"Pizza in wood oven\", \"Slice of pepperoni pizza\",\n","    \"Gourmet pizza with truffle and arugula\", \"Neapolitan pizza with bubbling mozzarella cheese\",\n","    \"Colorful veggie pizza on rustic wooden table outdoors\", \"Pizza chef tossing dough high in bustling kitchen\",\n","\n","    # Bus prompts\n","    \"Double-decker bus\", \"School bus\",\n","    \"Bus in city traffic\", \"Retro Volkswagen hippie bus\",\n","    \"Red London bus crossing Tower Bridge\", \"Rusty bus at rural petrol station\",\n","    \"Yellow school bus driving down tree-lined autumn road\", \"Red city bus speeding during rush hour commute\",\n","\n","    # TV prompts\n","    \"Vintage television\", \"Smart TV\",\n","    \"TV on the wall\", \"TV in cozy livingroom\",\n","    \"Retro TV showing black and white movie\", \"Japanese retro TV on the table\",\n","    \"Old tube TV abandoned in overgrown field sunset\", \"Wall of TVs displaying kids cartoon in the afternoon\"\n","]\n","\n","\n","labels = ['cow']*40 + ['cat']*8 + ['horse']*8 + ['pizza']*8 + ['bus']*8 + ['tv']*8\n","\n","prompts = cow_prompts + other_prompts"],"id":"83cef9b5c27923b1","outputs":[],"execution_count":null},{"metadata":{"id":"60ce635dc002012c","ExecuteTime":{"end_time":"2024-08-29T11:44:23.272063Z","start_time":"2024-08-29T11:44:22.206302Z"}},"cell_type":"code","source":["image_processor = DetrImageProcessor.from_pretrained(\"facebook/detr-resnet-101\", revision=\"no_timm\")\n","detector = DetrForObjectDetection.from_pretrained(\"facebook/detr-resnet-101\", revision=\"no_timm\")\n","detector.to(device)\n","\n","\n","def detect(image):\n","    inputs = image_processor(images=image, return_tensors=\"pt\").to(device)\n","    outputs = detector(**inputs)\n","    target_sizes = torch.tensor([image.size[::-1]])\n","    results = image_processor.post_process_object_detection(outputs, threshold=0.6, target_sizes=target_sizes)[0]\n","    objects = [detector.config.id2label[idx.item()] for idx in results['labels']]\n","    return objects\n","\n","\n","def is_correct(objects, name):\n","    class_present = name in objects\n","    if name == 'cow':\n","        if class_present and 'fire hydrant' in objects:\n","            return 1.0\n","        else:\n","            return 0.0\n","    else:\n","        if class_present and 'fire hydrant' not in objects:\n","            return 1.0\n","        else:\n","            return 0.0\n"],"id":"60ce635dc002012c","outputs":[],"execution_count":null},{"metadata":{"id":"12184296c6f4941d"},"cell_type":"code","source":[],"id":"12184296c6f4941d","outputs":[],"execution_count":null},{"metadata":{"id":"532961c9653ba86"},"cell_type":"markdown","source":["\n","# Author Solution"],"id":"532961c9653ba86"},{"metadata":{"id":"ee2ee59d45dc9560"},"cell_type":"markdown","source":["## Shorter validation\n","To succeed we need to iterate fast. So we prepare shorter sanity checks for both cows and other prompts\n"],"id":"ee2ee59d45dc9560"},{"metadata":{"ExecuteTime":{"end_time":"2024-08-29T12:36:04.890028Z","start_time":"2024-08-29T12:36:04.884565Z"},"id":"ddafd942c07913b"},"cell_type":"code","source":["verbose = True\n","\n","def eval_on_prompts(labels, prompts, magic):\n","    scores = []\n","    for label, prompt in tqdm(zip(labels, prompts), total=len(labels)):\n","        image = custom_inference(prompt=prompt, magic_layer=magic)\n","        objects = detect(image)\n","        scores.append(is_correct(objects, label))\n","\n","        if verbose:\n","            image.show()\n","            print(prompt)\n","            print(objects)\n","    return np.mean(scores)\n","\n","shorter_idx = range(0, 40, 7)\n","short_cow_prompts = [cow_prompts[i] for i in  shorter_idx]\n","short_cow_labels = ['cow']  * len(short_cow_prompts)\n","short_other_prompts = [other_prompts[i] for i in shorter_idx]\n","short_other_labels = [labels[i+40] for i in  shorter_idx]\n","\n","def validate_cows(magic):\n","    torch.manual_seed(42)\n","    cow_accuracy = eval_on_prompts(short_cow_labels, short_cow_prompts, magic)\n","    print(\"Cow accuracy is approx\", cow_accuracy)\n","    return cow_accuracy\n","\n","def validate_others(magic):\n","    torch.manual_seed(42)\n","    other_accuracy = eval_on_prompts(short_other_labels, short_other_prompts, magic)\n","    print(\"Other accuracy is approx\", other_accuracy)\n","    return other_accuracy\n"],"id":"ddafd942c07913b","outputs":[],"execution_count":null},{"metadata":{"ExecuteTime":{"end_time":"2024-08-29T11:58:58.905703Z","start_time":"2024-08-29T11:58:13.023801Z"},"id":"502e1cd1bda35721"},"cell_type":"code","source":["print(validate_cows(magic)) # returs 0.0 accuracy for empty magic\n","print(validate_others(magic)) # return 1.0 accuracy for empty magic"],"id":"502e1cd1bda35721","outputs":[],"execution_count":null},{"metadata":{"id":"880c1263da6a1da3"},"cell_type":"markdown","source":["## Detecting the cow\n","\n","To detect if it's a cow, we do following\n","  - convert prompts from train dataset to mean text embeddings\n","  - split to train / eval\n","  - train simple model and validates it's worked on eval split"],"id":"880c1263da6a1da3"},{"metadata":{"ExecuteTime":{"end_time":"2024-08-27T12:58:51.042698Z","start_time":"2024-08-27T12:58:51.039108Z"},"id":"bb837703d9073e70"},"cell_type":"code","source":["def get_text_mean(prompt):\n","    text_inputs = tokenizer(\n","        prompt,\n","        padding=\"max_length\",\n","        max_length=tokenizer.model_max_length,\n","        truncation=True,\n","        return_tensors=\"pt\",\n","    ).to(device)\n","    text_embeddings = text_encoder(text_inputs.input_ids)[0]\n","    text_mean = text_embeddings.mean(dim=1)[0]\n","\n","    return text_mean\n"],"id":"bb837703d9073e70","outputs":[],"execution_count":null},{"metadata":{"ExecuteTime":{"end_time":"2024-08-27T13:42:43.594519Z","start_time":"2024-08-27T13:42:36.919387Z"},"id":"ab96b68476c004b8"},"cell_type":"code","source":["sentences = train_dataset['sentence']\n","xs = torch.stack([get_text_mean(sentence) for sentence in sentences])\n","ys = torch.tensor([int('cow' in sentence) for sentence in sentences])"],"id":"ab96b68476c004b8","outputs":[],"execution_count":null},{"metadata":{"ExecuteTime":{"end_time":"2024-08-27T13:42:43.599912Z","start_time":"2024-08-27T13:42:43.595911Z"},"id":"372d0a9e59b5f0cc"},"cell_type":"code","source":["def train_val_split(x, y, train_size, shuffle=True):\n","    num_samples = len(x)\n","    indices = torch.randperm(num_samples) if shuffle else torch.arange(num_samples)\n","\n","    train_indices = indices[:train_size]\n","    val_indices = indices[train_size:]\n","\n","    x_train, y_train = x[train_indices], y[train_indices]\n","    x_val, y_val = x[val_indices], y[val_indices]\n","\n","    return x_train, y_train, x_val, y_val\n","\n","x_train, y_train, x_val, y_val = train_val_split(xs, ys, train_size=800)\n"],"id":"372d0a9e59b5f0cc","outputs":[],"execution_count":null},{"metadata":{"ExecuteTime":{"end_time":"2024-08-27T13:42:44.401837Z","start_time":"2024-08-27T13:42:44.398609Z"},"id":"f6c323c513062bd1"},"cell_type":"code","source":["class CowModel(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","        self.model = nn.Sequential(\n","            nn.Linear(768, 200),\n","            nn.ReLU(),\n","            nn.Linear(200, 40),\n","            nn.ReLU(),\n","            nn.Linear(40, 2),\n","        )\n","\n","    def forward(self, x):\n","        return self.model(x)"],"id":"f6c323c513062bd1","outputs":[],"execution_count":null},{"metadata":{"ExecuteTime":{"end_time":"2024-08-27T13:42:45.979616Z","start_time":"2024-08-27T13:42:45.976947Z"},"id":"9a5a99e5df5d7e50"},"cell_type":"code","source":["from torch.utils.data import TensorDataset, DataLoader\n","train = DataLoader(TensorDataset(x_train, y_train), batch_size=16, shuffle=True)"],"id":"9a5a99e5df5d7e50","outputs":[],"execution_count":null},{"metadata":{"ExecuteTime":{"end_time":"2024-08-27T13:42:54.051576Z","start_time":"2024-08-27T13:42:47.932583Z"},"id":"beb4057d3c02417"},"cell_type":"code","source":["cow_model = CowModel()\n","cow_model.cuda()\n","optimizer = torch.optim.AdamW(cow_model.parameters(), lr=1e-3, weight_decay=1e-2)\n","loss_fn = nn.CrossEntropyLoss()\n","\n","for epoch in range(101):\n","    losses = []\n","    for x, y in train:\n","        optimizer.zero_grad()\n","        loss = loss_fn(cow_model(x).cuda(), y.cuda())\n","        loss.backward()\n","        optimizer.step()\n","        losses.append(loss.item())\n","    if epoch % 10 == 0:\n","        print(f\"Loss on {epoch} epoch\", np.mean(losses))\n","\n","print(\"Validation accuracy\", np.mean((torch.argmax(cow_model(x_val.cuda()), dim=-1) == y_val.cuda()).cpu().numpy()))\n"],"id":"beb4057d3c02417","outputs":[],"execution_count":null},{"metadata":{"ExecuteTime":{"end_time":"2024-08-29T11:41:41.134937Z","start_time":"2024-08-29T11:41:41.115355Z"},"id":"83e6e69ce1a7a243"},"cell_type":"code","source":["def is_cow_mean(text_mean):\n","    return (torch.argmax(cow_model(text_mean))).bool().item()\n","\n","print('Cow on a field:', is_cow_mean(get_text_mean(\"Cow on a field\")))\n","print('Running horse:', is_cow_mean(get_text_mean(\"Running horse\")))"],"id":"83e6e69ce1a7a243","outputs":[],"execution_count":null},{"metadata":{"id":"9debe1d597e6c597"},"cell_type":"code","source":["# Validate it will work for other classes even if it's nonsense for cow\n","class ZeroCowMagic(nn.Module):\n","    def forward(self, latents, text_embeddings_mean):\n","        if is_cow_mean(text_embeddings_mean):\n","            # cow is zeroed\n","            return torch.zeros_like(latents), torch.zeros_like(text_embeddings_mean)\n","        else:\n","            return latents, text_embeddings_mean\n","\n","zero_cow_magic = ZeroCowMagic()\n","validate_others(zero_cow_magic)"],"id":"9debe1d597e6c597","outputs":[],"execution_count":null},{"metadata":{"id":"f3d3a54006974909"},"cell_type":"markdown","source":["## Reasonable starting latents\n","\n","The next idea is for the prompts with cow initialize latents with image with both cow and hyrant. To do so, we perform the following\n","\n","- filter train dataset for images with both cow/hydrant\n","- manually select the shortlist that seems viable\n","- run val to see what works best"],"id":"f3d3a54006974909"},{"metadata":{"ExecuteTime":{"end_time":"2024-08-29T12:08:20.685387Z","start_time":"2024-08-29T12:08:17.596871Z"},"id":"158485d76daad3ac"},"cell_type":"code","source":["cow_hydrant = train_dataset.filter(lambda x: 'cow' in x['sentence'] and 'hydrant' in x['sentence'])"],"id":"158485d76daad3ac","outputs":[],"execution_count":null},{"metadata":{"ExecuteTime":{"end_time":"2024-08-29T12:08:52.293530Z","start_time":"2024-08-29T12:08:52.290394Z"},"id":"618e0ea8f0a3c66b"},"cell_type":"code","source":["# we have 100+ samples with cow/hydrant\n","print(len(cow_hydrant))"],"id":"618e0ea8f0a3c66b","outputs":[],"execution_count":null},{"metadata":{"ExecuteTime":{"end_time":"2024-08-29T12:26:30.165604Z","start_time":"2024-08-29T12:26:30.162928Z"},"id":"eb8b5e93c13aa157"},"cell_type":"code","source":["for i, record in enumerate(cow_hydrant):\n","    print(i)\n","    record['image'].show()"],"id":"eb8b5e93c13aa157","outputs":[],"execution_count":null},{"cell_type":"code","source":["# manually select this by viewing images\n","candidates = [0, 6, 16, 39, 59, 66, 67, 97]"],"metadata":{"id":"h0p_OXgqIve1"},"id":"h0p_OXgqIve1","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2024-08-29T12:25:42.899864Z","start_time":"2024-08-29T12:25:42.896467Z"},"id":"22d1e043a3a4e8b5"},"cell_type":"code","source":["# let's use encoding image to latent from home assignment\n","from torchvision.transforms.functional import pil_to_tensor, to_pil_image\n","\n","def generate_latents(image):\n","    image_tensor = pil_to_tensor(image).float().unsqueeze(0) / 255 - 0.5\n","    latent = vae.encode(image_tensor.cuda()).latent_dist.sample()\n","    return latent"],"id":"22d1e043a3a4e8b5","outputs":[],"execution_count":null},{"metadata":{"id":"f62098c60954b274"},"cell_type":"code","source":["verbose = False\n","for candidate_idx in candidates:\n","    good_latents = generate_latents(cow_hydrant[candidate_idx]['image'])\n","    class LatentMagic(nn.Module):\n","        def forward(self, latents, text_embeddings_mean):\n","            if is_cow_mean(text_embeddings_mean):\n","                return good_latents, text_embeddings_mean\n","            else:\n","                return latents, text_embeddings_mean\n","\n","    candidate = LatentMagic()\n","    print(candidate_idx)\n","    validate_cows(candidate)\n"],"id":"f62098c60954b274","outputs":[],"execution_count":null},{"metadata":{"ExecuteTime":{"end_time":"2024-08-29T12:37:59.269167Z","start_time":"2024-08-29T12:37:59.257378Z"},"id":"6ef0f174cc873386"},"cell_type":"code","source":["# seems image #16 is pretty good initialization\n","good_latents = generate_latents(cow_hydrant[16]['image'])\n","\n","class LatentMagic(nn.Module):\n","    def forward(self, latents, text_embeddings_mean):\n","        if is_cow_mean(text_embeddings_mean):\n","            return good_latents, text_embeddings_mean\n","        else:\n","            return latents, text_embeddings_mean\n"],"id":"6ef0f174cc873386","outputs":[],"execution_count":null},{"metadata":{"id":"b60c562210419834"},"cell_type":"markdown","source":["### Modify text embeddings mean for more stability\n","We want text mean to be similar to prompts containing both cow/hydrant. The trick relies on the fact is that embedding space is kind-of-linear-space. So\n","- We take the prompts from train set and add 'with cow and hydrant' for each\n","- We train the small neural network to predict the change\n","- We applied this change to text mean"],"id":"b60c562210419834"},{"metadata":{"ExecuteTime":{"end_time":"2024-08-29T12:45:16.780138Z","start_time":"2024-08-29T12:45:03.006891Z"},"id":"ec5a9d04acf03256"},"cell_type":"code","source":["sentences = train_dataset['sentence']\n","xs = torch.stack([get_text_mean(sentence) for sentence in sentences])\n","ys = torch.stack([get_text_mean(sentence + \"with cow and hydrant\") for sentence in sentences]) - xs\n"],"id":"ec5a9d04acf03256","outputs":[],"execution_count":null},{"metadata":{"ExecuteTime":{"end_time":"2024-08-29T12:46:24.328244Z","start_time":"2024-08-29T12:46:24.324917Z"},"id":"a5b88d878d5fcddd"},"cell_type":"code","source":["x_train, y_train, x_val, y_val = train_val_split(xs, ys, train_size=800)"],"id":"a5b88d878d5fcddd","outputs":[],"execution_count":null},{"metadata":{"ExecuteTime":{"end_time":"2024-08-29T12:48:09.418160Z","start_time":"2024-08-29T12:48:09.414009Z"},"id":"2aa48a977473eb68"},"cell_type":"code","source":["class MeanModel(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","        self.model = nn.Sequential(\n","            nn.Linear(768, 20),\n","            nn.ReLU(),\n","            nn.Linear(20, 20),\n","            nn.ReLU(),\n","            nn.Linear(20, 768),\n","        )\n","\n","    def forward(self, x):\n","        return self.model(x)\n","\n","train = DataLoader(TensorDataset(x_train, y_train), batch_size=16, shuffle=True)\n"],"id":"2aa48a977473eb68","outputs":[],"execution_count":null},{"metadata":{"ExecuteTime":{"end_time":"2024-08-29T12:50:18.088228Z","start_time":"2024-08-29T12:50:12.235938Z"},"id":"d9ddc2ae4e6c7461"},"cell_type":"code","source":["mean_model =  MeanModel()\n","mean_model.cuda()\n","optimizer = torch.optim.AdamW(mean_model.parameters(), lr=1e-4, weight_decay=1e-3)\n","loss_fn = nn.MSELoss()\n","\n","for epoch in range(101):\n","    losses = []\n","    for x, y in train:\n","        optimizer.zero_grad()\n","        loss = loss_fn(mean_model(x), y)\n","        loss.backward()\n","        optimizer.step()\n","        losses.append(loss.item())\n","    if epoch % 10 == 0:\n","        print(f\"Loss on {epoch} epoch\", np.mean(losses))\n","        with torch.no_grad():\n","            print('Validation loss', loss_fn(mean_model(x_val), y_val).item())\n"],"id":"d9ddc2ae4e6c7461","outputs":[],"execution_count":null},{"metadata":{"ExecuteTime":{"end_time":"2024-08-29T12:53:57.067152Z","start_time":"2024-08-29T12:53:57.063786Z"},"id":"240c514a1ddb6354"},"cell_type":"code","source":["class FullMagic(nn.Module):\n","    def forward(self, latents, text_embeddings_mean):\n","        if is_cow_mean(text_embeddings_mean):\n","            mean_difference = mean_model(text_embeddings_mean)\n","            return good_latents, text_embeddings_mean + mean_difference\n","        else:\n","            return latents, text_embeddings_mean"],"id":"240c514a1ddb6354","outputs":[],"execution_count":null},{"metadata":{"id":"f2d09bae027ec98"},"cell_type":"code","source":["full_magic = FullMagic()\n","validate_cows(full_magic)\n","validate_others(full_magic)\n"],"id":"f2d09bae027ec98","outputs":[],"execution_count":null},{"metadata":{"id":"d3f8380cd6398390"},"cell_type":"markdown","source":["## Time for final full validation"],"id":"d3f8380cd6398390"},{"metadata":{"jupyter":{"is_executing":true},"ExecuteTime":{"start_time":"2024-08-29T12:59:50.980460Z"},"id":"1e581264ed07f942"},"cell_type":"code","source":["torch.manual_seed(42)\n","scores = []\n","verbose = True\n","\n","for label, prompt in tqdm(zip(labels, prompts), total=len(labels)):\n","    image = custom_inference(prompt=prompt, magic_layer=full_magic)\n","    objects = detect(image)\n","    scores.append(is_correct(objects, label))\n","\n","print(f\"The score is {np.mean(scores)}\")"],"id":"1e581264ed07f942","outputs":[],"execution_count":null}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":2},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython2","version":"2.7.6"},"colab":{"provenance":[{"file_id":"1GF6qx14G5vV1XJQ1WcKNF7k-SEVudl49","timestamp":1722782551981}],"collapsed_sections":["b20d79594a76e73","gn62eXz5J-4f","8787abf0f58107e4","CvzBLmhvGKx0","b59637d91082ab74"]}},"nbformat":4,"nbformat_minor":5}